{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "tracks with genre 113: 196\n",
      "16384\n",
      "2\n",
      "reshape: (?, 128, 128, 1)\n",
      "conv2d: (?, 128, 128, 32)\n",
      "maxpool2d: (?, 64, 64, 32)\n",
      "conv2d: (?, 64, 64, 32)\n",
      "maxpool2d: (?, 32, 32, 32)\n",
      "conv2d: (?, 32, 32, 32)\n",
      "maxpool2d: (?, 16, 16, 32)\n",
      "conv2d: (?, 16, 16, 32)\n",
      "maxpool2d: (?, 8, 8, 32)\n",
      "conv2d: (?, 8, 8, 64)\n",
      "maxpool2d: (?, 4, 4, 64)\n",
      "reshape: (?, 1024)\n",
      "wx+b: (?, 1024)\n",
      "relu: (?, 1024)\n",
      "wx+b: (?, 1024)\n",
      "relu: (?, 1024)\n",
      "wx+b: (?, 1024)\n",
      "relu: (?, 1024)\n",
      "dropout: (?, 1024)\n",
      "wx+b: (?, 2)\n",
      "extracted 128 samples from 128.0 tracks (matches: 68)\n",
      "extracted 128 samples from 128.0 tracks (matches: 78)\n",
      "extracted 128 samples from 128.0 tracks (matches: 81)\n",
      "Iter 384, Minibatch Loss= 89033457664.000000, Training Accuracy= 0.61719\n",
      "extracted 128 samples from 128.0 tracks (matches: 72)\n",
      "extracted 128 samples from 128.0 tracks (matches: 77)\n",
      "extracted 128 samples from 128.0 tracks (matches: 74)\n",
      "Iter 768, Minibatch Loss= 67829116928.000000, Training Accuracy= 0.43750\n",
      "extracted 128 samples from 128.0 tracks (matches: 73)\n",
      "extracted 128 samples from 128.0 tracks (matches: 72)\n",
      "extracted 128 samples from 128.0 tracks (matches: 84)\n",
      "Iter 1152, Minibatch Loss= 35215953920.000000, Training Accuracy= 0.57812\n",
      "extracted 128 samples from 128.0 tracks (matches: 78)\n",
      "extracted 128 samples from 128.0 tracks (matches: 74)\n",
      "extracted 128 samples from 128.0 tracks (matches: 82)\n",
      "Iter 1536, Minibatch Loss= 37203660800.000000, Training Accuracy= 0.59375\n",
      "extracted 128 samples from 128.0 tracks (matches: 70)\n",
      "extracted 128 samples from 128.0 tracks (matches: 81)\n",
      "extracted 128 samples from 128.0 tracks (matches: 83)\n",
      "Iter 1920, Minibatch Loss= 25307615232.000000, Training Accuracy= 0.64844\n",
      "extracted 128 samples from 128.0 tracks (matches: 76)\n",
      "extracted 128 samples from 128.0 tracks (matches: 82)\n",
      "extracted 128 samples from 128.0 tracks (matches: 73)\n",
      "Iter 2304, Minibatch Loss= 27284408320.000000, Training Accuracy= 0.49219\n",
      "extracted 128 samples from 128.0 tracks (matches: 75)\n",
      "extracted 128 samples from 128.0 tracks (matches: 86)\n",
      "extracted 128 samples from 128.0 tracks (matches: 92)\n",
      "Iter 2688, Minibatch Loss= 21761826816.000000, Training Accuracy= 0.52344\n",
      "extracted 128 samples from 128.0 tracks (matches: 87)\n",
      "extracted 128 samples from 128.0 tracks (matches: 82)\n",
      "extracted 128 samples from 128.0 tracks (matches: 83)\n",
      "Iter 3072, Minibatch Loss= 25598459904.000000, Training Accuracy= 0.63281\n",
      "extracted 128 samples from 128.0 tracks (matches: 78)\n",
      "extracted 128 samples from 128.0 tracks (matches: 79)\n",
      "extracted 128 samples from 128.0 tracks (matches: 71)\n",
      "Iter 3456, Minibatch Loss= 17282359296.000000, Training Accuracy= 0.54688\n",
      "extracted 128 samples from 128.0 tracks (matches: 81)\n",
      "extracted 128 samples from 128.0 tracks (matches: 81)\n",
      "extracted 128 samples from 128.0 tracks (matches: 80)\n",
      "Iter 3840, Minibatch Loss= 17865347072.000000, Training Accuracy= 0.46094\n",
      "extracted 128 samples from 128.0 tracks (matches: 86)\n",
      "extracted 128 samples from 128.0 tracks (matches: 74)\n",
      "extracted 128 samples from 128.0 tracks (matches: 78)\n",
      "Iter 4224, Minibatch Loss= 11939503104.000000, Training Accuracy= 0.59375\n",
      "extracted 128 samples from 128.0 tracks (matches: 90)\n",
      "extracted 128 samples from 128.0 tracks (matches: 77)\n",
      "extracted 128 samples from 128.0 tracks (matches: 86)\n",
      "Iter 4608, Minibatch Loss= 16816423936.000000, Training Accuracy= 0.59375\n",
      "extracted 128 samples from 128.0 tracks (matches: 80)\n",
      "extracted 128 samples from 128.0 tracks (matches: 78)\n",
      "extracted 128 samples from 128.0 tracks (matches: 79)\n",
      "Iter 4992, Minibatch Loss= 16692993024.000000, Training Accuracy= 0.49219\n",
      "extracted 128 samples from 128.0 tracks (matches: 82)\n",
      "extracted 128 samples from 128.0 tracks (matches: 80)\n",
      "extracted 128 samples from 128.0 tracks (matches: 81)\n",
      "Iter 5376, Minibatch Loss= 10542095360.000000, Training Accuracy= 0.55469\n",
      "extracted 128 samples from 128.0 tracks (matches: 77)\n",
      "extracted 128 samples from 128.0 tracks (matches: 67)\n",
      "extracted 128 samples from 128.0 tracks (matches: 78)\n",
      "Iter 5760, Minibatch Loss= 11677577216.000000, Training Accuracy= 0.56250\n",
      "extracted 128 samples from 128.0 tracks (matches: 82)\n"
     ]
    }
   ],
   "source": [
    "from helpers.extractor import *\n",
    "from helpers.deezer_tools import *\n",
    "from helpers.neural_network import *\n",
    "\n",
    "loader = DeezerLoader(fixed_size=128*128, sample_size=1/32, local=True, limit_genres=[113], other_genres_rate=0.5)\n",
    "loader.shuffle_rate = 1\n",
    "loader.picker.SortAllTracks()\n",
    "\n",
    "print(len(loader.picker.tracks))\n",
    "print(\"tracks with genre 113: \"+str(len(loader.picker.tracksByGenre[113])))\n",
    "\n",
    "network = ConvNet(loader, training_iters=128*10000, display_step=3)\n",
    "print(network.n_input)\n",
    "print(network.n_classes)\n",
    "\n",
    "final_width = int(network.final_img_width/8)\n",
    "\n",
    "layers = []\n",
    "layers.append(NNOperation(\"reshape\", [-1, network.img_width, network.img_width, 1]))\n",
    "layers.append(NNOperation(\"conv2d\", [5, 5, 1, 32]))\n",
    "layers.append(NNOperation(\"maxpool2d\", 2))\n",
    "layers.append(NNOperation(\"conv2d\", [5, 5, 32, 32]))\n",
    "layers.append(NNOperation(\"maxpool2d\", 2))\n",
    "layers.append(NNOperation(\"conv2d\", [5, 5, 32, 32]))\n",
    "layers.append(NNOperation(\"maxpool2d\", 2))\n",
    "layers.append(NNOperation(\"conv2d\", [5, 5, 32, 32]))\n",
    "layers.append(NNOperation(\"maxpool2d\", 2))\n",
    "layers.append(NNOperation(\"conv2d\", [5, 5, 32, 64]))\n",
    "layers.append(NNOperation(\"maxpool2d\", 2))\n",
    "layers.append(NNOperation(\"reshape\", [-1, final_width*final_width*64]))\n",
    "layers.append(NNOperation(\"wx+b\", [final_width*final_width*64, 1024], [1024]))\n",
    "layers.append(NNOperation(\"relu\"))\n",
    "layers.append(NNOperation(\"dropout\", network.dropout))\n",
    "layers.append(NNOperation(\"wx+b\", [1024, 1024], [1024]))\n",
    "layers.append(NNOperation(\"relu\"))\n",
    "layers.append(NNOperation(\"dropout\", network.dropout))\n",
    "layers.append(NNOperation(\"wx+b\", [1024, 1024], [1024]))\n",
    "layers.append(NNOperation(\"relu\"))\n",
    "layers.append(NNOperation(\"dropout\", network.dropout))\n",
    "layers.append(NNOperation(\"wx+b\", [1024, network.n_classes], [network.n_classes]))\n",
    "\n",
    "network.Run(layers)#save_path=\"temp/model.ckpt\", restore_path=\"temp/model.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from helpers.extractor import *\n",
    "from helpers.neural_network import *\n",
    "\n",
    "loader = CifarLoader(\"inputs/datasets/cifar10/\", \"Grayscale\")\n",
    "network = ConvNet(loader, learning_rate=0.0001)\n",
    "\n",
    "final_width = network.final_img_width\n",
    "\n",
    "layers = []\n",
    "layers.append(NNOperation(\"reshape\", [-1, network.img_width, network.img_width, 1]))\n",
    "layers.append(NNOperation(\"conv2d\", [5, 5, 1, 32]))\n",
    "layers.append(NNOperation(\"maxpool2d\", 2))\n",
    "layers.append(NNOperation(\"conv2d\", [5, 5, 32, 64]))\n",
    "layers.append(NNOperation(\"maxpool2d\", 2))\n",
    "layers.append(NNOperation(\"reshape\", [-1, final_width*final_width*64]))\n",
    "layers.append(NNOperation(\"wx+b\", [final_width*final_width*64, 1024], [1024]))\n",
    "layers.append(NNOperation(\"wx+b\", [1024, 1024], [1024]))\n",
    "layers.append(NNOperation(\"wx+b\", [1024, 1024], [1024]))\n",
    "layers.append(NNOperation(\"relu\"))\n",
    "layers.append(NNOperation(\"dropout\", network.dropout))\n",
    "layers.append(NNOperation(\"wx+b\", [1024, network.n_classes], [network.n_classes]))\n",
    "\n",
    "network.Run(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/minst_folder/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/minst_folder/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/minst_folder/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/minst_folder/t10k-labels-idx1-ubyte.gz\n",
      "reshape: (?, 28, 28, 1)\n",
      "conv2d: (?, 28, 28, 32)\n",
      "maxpool2d: (?, 14, 14, 32)\n",
      "conv2d: (?, 14, 14, 64)\n",
      "maxpool2d: (?, 7, 7, 64)\n",
      "reshape: (?, 3136)\n",
      "wx+b: (?, 1024)\n",
      "relu: (?, 1024)\n",
      "dropout: (?, 1024)\n",
      "wx+b: (?, 10)\n",
      "Iter 1280, Minibatch Loss= 56356.902344, Training Accuracy= 0.13281\n",
      "Iter 2560, Minibatch Loss= 23915.062500, Training Accuracy= 0.28906\n",
      "Iter 3840, Minibatch Loss= 21796.224609, Training Accuracy= 0.41406\n",
      "Iter 5120, Minibatch Loss= 13265.935547, Training Accuracy= 0.53906\n"
     ]
    }
   ],
   "source": [
    "from helpers.extractor import *\n",
    "from helpers.neural_network import *\n",
    "\n",
    "loader = MinstLoader(\"inputs/datasets/MNIST_data/\")\n",
    "network = ConvNet(loader)\n",
    "\n",
    "network.Run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
